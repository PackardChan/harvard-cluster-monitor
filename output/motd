
!!!!!!!!!!!!!!!!!!!!!!!!!    Cannon    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Welcome to Cannon, an HPC resource for the research community,
hosted by Research Computing at HU's Faculty of Arts and Sciences.

+------------------- Helpful Documentation: --------------------+
| https://docs.rc.fas.harvard.edu/kb/quickstart-guide/          |
| https://docs.rc.fas.harvard.edu/kb/running-jobs/              |
| https://docs.rc.fas.harvard.edu/kb/convenient-slurm-commands/ |
+---------------------------------------------------------------+

+------------------- NEWS & UPDATES: ----------------------------------------------------+
+ OFFICE HOURS: Wednesdays noon-3pm, see website for details                             +
+ Check our consulting calendar at: https://www.rc.fas.harvard.edu/consulting-calendar/  +
+ Check our training schedule at: https://www.rc.fas.harvard.edu/upcoming-training/      +
+----------------------------------------------------------------------------------------+

https://www.rc.fas.harvard.edu/maintenance

NEXT MAINTENANCE: MAY 23RD 6PM - MAY 26TH 12PM

###################IMPORTANT####################

MGHPCC POWER MAINTENANCE: Our annual power maintenance is
slated to run May 23rd 6pm - May 26th 12pm.  All cluster
jobs will be terminated and the cluster will be
unavailable for the duration. Plan accordingly
and see: 

https://www.rc.fas.harvard.edu/events/mghpcc-power-shutdown-2022/

OFFICE HOURS: Are held on Zoom from 12-3PM on Wednesdays.
See https://www.rc.fas.harvard.edu/training/office-hours/ for details.

SURVEY: Please fill out the 2022 Cluster Users survey.
Thanks in advance for the feedback!

https://harvard.az1.qualtrics.com/jfe/form/SV_72jkHXNL2arI34i  

PUBLICATIONS: Do you have a publication which used one
of the FASRC Clusters? Let us know so we can add it to
our publication list:

https://www.rc.fas.harvard.edu/cluster/publications/

You can also find example text for acknowledging use
of the cluster.

HOLYLFS/BOSLFS: Please only use holylfs/boslfs for
long term storage and use holyscratch01 for production work.
We will be in contact with labs when we are ready to move
labs on these systems to new storage.


